name: random_coarse_gaussian_terrain_with_random_start_goal_stabilization
comment: "Random coarse gaussian terrain with random start and goal, parameters derived from the best Optuna run for enforcing control smoothness"
seed: 42
use_wandb: true
eval_repeats_after_training: 1
use_tensorboard: false
device: cuda
training_dtype: ${dtype:float32}
total_frames: ${mul:5242880,4}
engine_iters_per_env_step: 4
num_robots: 256
time_steps_per_batch: 128
epochs_per_batch: 5
frames_per_sub_batch: 4096
eval_and_save_every: 10
max_eval_steps: 500
optimizer: ${cls:torch.optim.AdamW}
scheduler: ${cls:torch.optim.lr_scheduler.LinearLR}
scheduler_opts:
  start_factor: 1.0
  end_factor: 0.28703006303809225
  total_iters: ${intdiv:${total_frames},${mul:${time_steps_per_batch},${num_robots}}}
max_grad_norm: 0.5
grid_res: 0.05
max_coord: 3.2
heightmap_gen: ${ cls:flipper_training.heightmaps.multi_gaussian.MultiGaussianHeightmapGenerator}
heightmap_gen_opts:
  min_gaussians: 100
  max_gaussians: 200
  min_height_fraction: 0.1
  max_height_fraction: 0.2
  min_std_fraction: 0.1
  max_std_fraction: 0.18
  min_sigma_ratio: 0.6
  top_height_percentile_cutoff: 0.7
world_opts:
  k_stiffness: 30000
  k_friction_lon: 1.0
  k_friction_lat: 0.8
engine_opts:
  damping_alpha: 5.0
  dt: 0.007
engine_compile_opts:
  options:
    triton.cudagraphs: true
  fullgraph: true
  correctness_iters: 10
  benchmark_iters: 1000
  atol: 0.1
  rtol: 1.0e-06
robot_model_opts:
  kind: marv
  mesh_voxel_size: 0.01
  points_per_driving_part: 384
  points_per_body: 512
  wheel_assignment_margin: 0.02
  linear_track_assignment_margin: 0.05
vecnorm_opts:
  decay: 0.99
  eps: 0.0001
vecnorm_on_reward: true
observations:
  - cls: ${cls:flipper_training.observations.robot_state.LocalStateVector}
    opts:
      apply_noise: true
      noise_scale: 1.0e-05
      encoder_opts:
        num_hidden: 2
        hidden_dim: 64
        output_dim: 64
        layernorm: true
  - cls: ${cls:flipper_training.observations.heightmap.Heightmap}
    opts:
      apply_noise: true
      noise_scale: 1.0e-05
      percep_shape:
        - 64
        - 64
      percep_extent:
        - 1.0
        - 1.0
        - -1.0
        - -1.0
      interval:
        - -1.0
        - 1.0
      normalize_to_interval: false
      encoder_opts:
        output_dim: 64
gae_opts:
  gamma: 0.999
  lmbda: 0.95
  average_gae: false
  skip_existing: false
ppo_opts:
  clip_epsilon: 0.2
  entropy_bonus: true
  entropy_coef: 0.02
  critic_coef: 1.0
  loss_critic_type: smooth_l1
  normalize_advantage: true
  separate_losses: false
data_collector_opts:
  compile_policy: false
  split_trajs: false
  exploration_type: RANDOM
objective: ${cls:flipper_training.rl_objectives.random_nav.RandomNavigationObjective}
objective_opts:
  goal_reached_threshold: 0.2
  start_z_offset: 0.3
  goal_z_offset: 0.2
  iteration_limit_factor: 5
  min_dist_to_goal: 2.0
  max_dist_to_goal: 5.0
  max_feasible_pitch: 1.5
  max_feasible_roll: 1.5
  start_position_orientation: towards_goal
  higher_allowed: 0.3
  cache_size: 30000
  init_joint_angles: random
reward: ${cls:flipper_training.rl_rewards.rewards.PotentialGoalWithPenaltiesConfigurable}
reward_opts:
  goal_reached_reward: 7.963765459661249
  failed_reward: -7.737836592755915
  potential_coef: 38.23252716396916
  gamma: ${gae_opts.gamma}
  step_penalty: -0.37878885469543
  joint_vel_variance_coef: 0.12175687152820583
  joint_angle_variance_coef: 0.08011397434183266
  track_vel_variance_coef: 0.2876607076073408
  roll_coef: 0.1
  roll_rate_coef: 0.1
  pitch_coef: 0.1
  pitch_rate_coef: 0.1
policy_config: ${cls:flipper_training.policies.mlp_policy.MLPPolicyConfig}
policy_opts:
  share_encoder: false
  apply_baselines_init: true
  actor_mlp_opts:
    num_hidden: 2
    hidden_dim: 64
    layernorm: false
  value_mlp_opts:
    num_hidden: 2
    hidden_dim: 64
    layernorm: true
  actor_optimizer_opts:
    lr: 0.0004824297168107223
    weight_decay: 0.0001
  value_optimizer_opts:
    lr: 0.0023065521662259366
    weight_decay: 0.0001
policy_weights_path: runs/ppo/random_trunk_with_noise_sota_no_groupnorm_2025-05-08_15-49-19/weights/policy_final.pth
vecnorm_weights_path: runs/ppo/random_trunk_with_noise_sota_no_groupnorm_2025-05-08_15-49-19/weights/vecnorm_final.pth
