# General Experiment Configuration
type: ${cls:flipper_training.experiments.ppo.config.PPOExperimentConfig}
name: deterministic_trunk_debug_for_hparam_calibration
comment: "Debugging PPO with deterministic trunk heightmap generation"
seed: 42
use_wandb: true
device: cuda:0

# Training Loop Parameters
training_dtype: ${dtype:float32}
total_frames: ${mul:5242880,2}
frames_per_batch: 128
epochs_per_batch: 1
frames_per_sub_batch: 64 # Must divide frames_per_batch * num_robots
eval_and_save_every: 10 # steps
max_eval_steps: 800

# Optimizer and Scheduler Configuration
optimizer: ${cls:torch.optim.AdamW}
optimizer_opts:
  actor:
    lr: 1e-4
    weight_decay: 1e-3
  critic:
    lr: 3e-4
    weight_decay: 1e-3
scheduler: ${cls:torch.optim.lr_scheduler.StepLR}
scheduler_opts:
  step_size: ${intdiv:${intdiv:${total_frames},${mul:${frames_per_batch},${num_robots}}},2}
  gamma: 0.5
max_grad_norm: 0.5

# Environment and Simulation Settings
num_robots: 128
grid_res: 0.05
max_coord: 3.2
heightmap_gen: ${ cls:flipper_training.heightmaps.trunks.FixedTrunkHeightmapGenerator}
heightmap_gen_opts:
  trunk_height: 0.2
  trunk_width: 0.5
world_opts:
  k_stiffness: 30000
  k_friction_lon: 0.8
  k_friction_lat: 0.5
engine_opts:
  damping_alpha: 5.0
  dt: 0.01
engine_compile_opts:
  max-autotune: true
  triton.cudagraphs: true
  coordinate_descent_tuning: true
  correctness_iters: 100
  benchmark_iters: 1000
  atol: 1
  rtol: 0

# Robot Model Configuration
robot_model_opts:
  kind: marv
  mesh_voxel_size: 0.01
  points_per_driving_part: 384
  points_per_body: 512
  wheel_assignment_margin: 0.02
  linear_track_assignment_margin: 0.05

# Observation and Normalization Settings
vecnorm_opts:
  decay: 0.99
  eps: 1e-4
vecnorm_on_reward: true
observations:
  state:
    observation: ${cls:flipper_training.observations.robot_state.LocalStateVector}
    opts: {}
  heightmap:
    observation: ${cls:flipper_training.observations.heightmap.Heightmap}
    opts:
      percep_shape: [64, 64]
      percep_extent: [1.0, 1.0, -1.0, -1.0]
observation_encoders_opts:
  state:
    output_dim: 32
    hidden_dim: 32
    num_hidden: 1
    ln: true
  heightmap:
    output_dim: 32

# PPO Algorithm Parameters
gae_opts:
  gamma: 0.99
  lmbda: 0.95
  average_gae: false
  skip_existing: false
gae_compile_opts: null # Optional compilation settings for GAE calculation
ppo_opts:
  clip_epsilon: 0.2
  entropy_bonus: true
  entropy_coef: 0.02
  critic_coef: 1.0
  loss_critic_type: smooth_l1
  normalize_advantage: false
ppo_compile_opts: null # Optional compilation settings for PPO
data_collector_opts:
  split_trajs: false
  compile_policy: true
  exploration_type: RANDOM

# Task Objective Configuration
objective: ${cls:flipper_training.rl_objectives.fixed_goal.FixedStartGoalNavigation}
objective_opts:
  start_x_y_z: ${tensor:[-1.5, 0.0, 0.2]}
  goal_x_y_z: ${tensor:[1.5, 0.0, 0.05]}
  iteration_limit: 800
  max_feasible_pitch: 1.5
  max_feasible_roll: 1.5
  goal_reached_threshold: 0.1
  init_joint_angles: ${tensor:[0.0, 0.0, 0.0, 0.0]}

# Reward Function Configuration
reward: ${cls:flipper_training.rl_rewards.rewards.PotentialGoalWithConditionalVelocityBonus}
reward_opts:
  goal_reached_reward: 200.0
  failed_reward: -200.0
  step_penalty: -5e-1
  gamma: ${gae_opts.gamma} # Should match GAE gamma
  potential_coef: 20.0
  velocity_bonus_coef: 0.1

# Policy and Value Network Architecture
policy_opts:
  hidden_dim: 32
  num_hidden: 1
  ln: false
value_function_opts:
  hidden_dim: 32
  num_hidden: 1
  ln: false
