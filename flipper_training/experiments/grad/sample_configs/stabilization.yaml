type: ${cls:flipper_training.experiments.ppo.config.PPOExperimentConfig}
name: first_ppo_experiment
comment: First PPO experiment
training_dtype: ${dtype:float32}
seed: 42
use_wandb: true
max_eval_steps: 1000
device: cpu
num_robots: 16
save_weights_every: 10
grid_res: 0.05
max_coord: 3.2
learning_rate: 0.001
norm_init_iters: 1000
robot_model_opts:
  kind: marv
  mesh_voxel_size: 0.01
  points_per_driving_part: 384
  points_per_body: 512
  wheel_assignment_margin: 0.02
  linear_track_assignment_margin: 0.05
optimizer: ${cls:torch.optim.Adam}
optimizer_opts:
  lr: 0.001
scheduler: ${cls:torch.optim.lr_scheduler.CosineAnnealingLR}
total_frames: 5242880
frames_per_batch: 128
scheduler_opts:
  T_max: ${intdiv:${total_frames},${mul:${frames_per_batch},${num_robots}}}
max_grad_norm: 1
heightmap_gen: ${ cls:flipper_training.heightmaps.multi_gaussian.MultiGaussianHeightmapGenerator}
heightmap_gen_opts:
  min_gaussians: 400
  max_gaussians: 600
  min_height_fraction: 0.03
  max_height_fraction: 0.12
  min_std_fraction: 0.03
  max_std_fraction: 0.08
  min_sigma_ratio: 0.6
world_opts:
  k_stiffness: 30000
  k_friction_lon: 0.8
  k_friction_lat: 0.5
engine_compile_opts:
  max-autotune: true
  triton.cudagraphs: true
  coordinate_descent_tuning: true
  correctness_iters: 100
  benchmark_iters: 1000
  atol: 1e-1
  rtol: 1e-6
evaluate_every: 10
epochs_per_batch: 5
frames_per_sub_batch: 64
gae_compile_opts: null
ppo_compile_opts: null
gae_opts:
  gamma: 0.99
  lmbda: 0.95
  average_gae: true
  skip_existing: false
ppo_opts:
  clip_epsilon: 0.2
  entropy_bonus: true
  entropy_coef: 0.01
  critic_coef: 1.0
  loss_critic_type: smooth_l1
data_collector_opts:
  split_trajs: false
  compile_policy: true
  exploration_type: RANDOM
objective: ${cls:flipper_training.rl_objectives.simple_stab.SimpleStabilizationObjective}
objective_opts:
  higher_allowed: 0.8
  min_dist_to_goal: 0.7
  max_dist_to_goal: 1.0
  start_z_offset: 0.3
  iteration_limit_factor: 10
  start_position_orientation: towards_goal
  goal_reached_threshold: 0.05
  max_feasible_roll: 1.5
  cache_size: 10000
reward: ${cls:flipper_training.rl_rewards.rewards.RollPitchGoal}
reward_opts:
  goal_reached_reward: 1000.0
  failed_reward: -1000.0
  omega_weight: 5.0
  goal_weight: 3.0
observations:
  perception:
    observation: ${cls:flipper_training.observations.heightmap.Heightmap}
    opts:
      percep_shape:
        - 128
        - 128
      percep_extent:
        - 1.0
        - 1.0
        - -1.0
        - -1.0
  observation:
    observation: ${cls:flipper_training.observations.robot_state.RobotStateVector}
    opts: {}
policy_opts:
  hidden_dim: 64
  value_mlp_layers: 2
  actor_mlp_layers: 2
engine_opts:
  damping_alpha: 5.0
